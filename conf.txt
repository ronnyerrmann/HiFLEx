## How to used the Scripts
# prepare filelist: run prepare_file_list.py
# every day: run HiFLEx.py   : shift orders, create wavelength solution for the day, create normalised extracted spectrum, extract the spectrum

# Configuration file for:
#   create_badpx_mask.py
#   prepare_file_list.py
#   hiflex.py
#   remove_orders.py

## Format of the parameters:
#---------------------------
#   - number of white spaces doesn't matter
#   - lists need to be given with comma between the values. The brakets [] are optional
#   - any line containing a '#' at any position will be handled as comment and not be used by the script
#   - if the same parameter is defined several times in the file, the latest verion will be used


  ## General parameters:
  #---------------------
  # GUI: allows userinput for some of the steps (tracing the echelle orders)
GUI = True


  # The folder where the raw data is stored. All file names given in this section need to be given relative to this path, e.g. with leading subfolder if data is stored in a subfolder.
  #         Please note that NO SPACEs and NO COMMAs are allowed in the path structure
  #         It is possible to give a list of sources
raw_data_paths           = /local/home/ronny/data_EXOhSPEC/20200813/


  # To extract the data correctly, the frame should be rotated in the way, that traces (dispersion axis) are along vertical direction
  #           with wavelength increasing with increasing pixel number (blue on top). 
  #           If necessary the image can be flipped after the rotation so that the central wavelength of the orders decreases with increasing 
  #           pixel number (blue orders on the right of the (rotated) image and the red orders on the left).
  #               format: multiple of 90
  # for QSI (both), Photometrics:     rotate_frame = 90
  # for Moravian, SBIG:               rotate_frame = 180 or rotate_frame = 0; flip_frame = True
  # for HARPS:                        rotate_frame = 180
  # for MRES:                         rotate_frame = 270, flip_frame = False
rotate_frame    = 90
flip_frame      = False


  # subframe: gives the size in x (dispersion axis) and y (cross-dispersion axis), and the starting coordinates in x and y.
  #           The numbering of the pixel starts with zero.
  #           The image will be extended if sizes bigger than the image are given. This includes the starting coordinates
  #           possible formats: can be empty, or
  #                             [4250,2838,0,0], or
  #                             4250,2838,0,0
  # for QSI:              subframe        = [4250,1600,0,450]
  # for Moravian:         subframe        = [2500,3056,56,0]
  # for SBIG:             subframe        = [2750,4096,350,0]
  # for HARPS:            subframe        = [4096,2048,0,50]
  # for MRES:             subframe        = [1201,511,380,0]
subframe        = []


  ## If the positions of the traces between consecutive observations has changed too much, the orders are searched from scratch and then adjusted.
  # bin_search_apertures: Search the apertures of the orders in a binned image. The first value gives the binning along one order (vertical axis, dispersion axis)
  #                 and the second value the binning perpendicular to the orders (horizontal, cross-dispersion axis). 
  #                 Heavy binning along the dispersion axis is possible in this step, while binning in cross-dispersion needs to be significantly smaller than the minimum separation between orders.
  #                 The area binned together must not contain more background pixel than pixels on which the traces fall, as the median will make the traces disappear otherwise.
  #                 format: list of two integers
  # for QSI:              bin_search_apertures = 20,5
  # for Moravian, SBIG:   bin_search_apertures = 20,4
  # for HARPS:            bin_search_apertures = 20,2
bin_search_apertures    = 20,2
  # bin_adjust_apertures: Adjust the center of the apertures in a less binned image. Description as for bin_search_apertures. The second value should be 1 to reach best precission
  #                 format: list of two integers
bin_adjust_apertures    = 3,1
  # polynom_order_apertures: Order of the polynomial to define the form of the traces along the CCD
  #                format: integer; standard: 5
polynom_order_apertures = 5
  # width_percentile: At what percentage of the height of the trace should the boundaries (width) of the order be defined? This is important for extraction.
  #                width_percentile = 50 coresponds to the FWHM; width_percentile = 5 corresponds to nearly all light being used
  #                format: integer; standard: 5
width_percentile        = 05

  ## Between observations of different nights the positions of the traces along cross-dispersion axis might change. The pipeline will determine the shift
  # Path and filename to the file with the previous found positions of the orders.
  #   If the files don't exist, a solution is created from scratch 
original_master_traces_filename = /data/ronny/Reduced/2019NA/master_traces_sci.fits
  # maxshift gives the maximum allowed shift between the current file defined in "master_trace_sci_filename" and the file given in "original_master_traces_filename".
  #           format: float or integer
maxshift_orders = 15
  # Also the width of the traces might change. As the measurements are only done on a subsample of the data, the user needs to decide if the new, less precise widths should be used.
update_width_orders = False


  ## Finding the traces of the calibration fiber 
  # arcshift_side: On which side of the science orders are the corresponding calibration orders located. Possible values are 
  #               "left" (towards the redder order), "right" (to the bluer order), or "center" (same position as the science orders)
arcshift_side = left
  # arcshift_range: Range to search for the shift between the flat traces and arc_traces. This value is determinded by the the script, only in case the process fails the parameter is used.
arcshift_range  = [25, 60]


  ## The following parameter define the adjustments made for the wavelength solution. Thereby the process starts with a rugh wavelength solution and ends with a much better one. 5 steps with 10 iterations each are done
  # polynom_order_traces: order of a polynomial fit for rough and final solution to fit the wavelength against line position in each order. 
  #                       For a setup not covering a full order [2,4] should be fine. a final value of 3 is not enough to reflect the position of the lines on the CCD
  #                       For a setup covering a full order or more than 20 orders [2,5] might be necessary
  #                 format: list of two integers
polynom_order_traces = [4,4]
  # polynom_order_intertrace: order for rough and final solution to fit the parameters of the polynomials between the orders. Normally 2 should be fine
  #                 format: list of one or two integers
polynom_order_intertraces = 4
  # opt_px_range: What fraction around the central pixel of each order should be used to corellate lines in the emmission line (arc) spectrum with the reference spectrum. Normally [0.9, 1.0] is good.
  #               If the previous solution differs a lot at the edges of the orders, than the first value should be smaller.
  #               If the previous solution matches nearly exactly the current solution, then both value can be bigger, e.g. [0.8,2]
opt_px_range = [0.9, 1.0]
  # diff_pxs: What is the maximum difference between wavelength solution and reference line in order to assigned the lines together.
  #           A good value is 8px, can be higher as sigma-clipping is performed to get rid of badly assigned lines. The Gaussian width of the emission lines is used in the final asignment
diff_pxs = 8

  ## The following parameters define the search area when finding the new wavelength solution for the night by using an old solution. 
  ##          The smaller the ranges, the faster the calculation, but the less flexible to changes. However, too big values might lead to the wrong local minimum.
  # order_offset: Search range to find out by how many orders are the two solutions shifted. Normally the change should be less than 1 Order. The offset will be negative, if redder orders are added.
  #               Gives the range of possible extra or fewer red orders, normally [-1, +1] should be fine
order_offset = [-1,1]
  # px_offset: search range and step size to find out by how much the central pixel of all orders was shifted compared to the previous wavelength solution.
  #               Normally the change should be less than 100 pixel. Sign is the pixel shift from old to new. Gives the range and step size. Standard: [-100,100,10].
px_offset = [-100, 100, 10]
  # px_offset_order: Additional shift [in pixel per oder] between the orders in case big variations happened. Standard: [-0.4, 0.4, 0.2]
px_offset_order = [-0.4, 0.4, 0.2]
  # resultion_offset_pct: Percentage of how much the resolution between the solutions changes. The value is applied in both directions, in total 11 steps will be made. Normally 1 percent should be fine.
resolution_offset_pct = 1
  # Path and filename to the file with the previous wavelength solution.
  #   If the files don't exist, a solution is created from scratch: File pixel_to_wavelength.txt needs to be created in order to create the wavelength solution. 
  #                                                                        The wavelength solution created from pixel_to_wavelength.txt will be saved in original_master_wavelensolution_filename.
  #                                                                If no wavelength solution is necessay, it can be set to "pseudo".
original_master_wavelensolution_filename = /data/ronny/Reduced/20190205/master_wavelength_sci.fits

  ## To determine the background
  # background_width_multiplier defines the area which is excluded for determining the background, e.g. to remove the scattered light.
  # It has the same settings as extraction_width_multiplier/arcextraction_width_multiplier.
  # The first value is for the science and the second for the calibration traces
  #                 format: list of two floats
background_width_multiplier = [1.2, 0.5]
  # polynom_bck defines the orders for a 2d polynomial that is fitted to the background. The first value is for dispersion and the second for cross-dispersion direction.
  #                 format: list of two integers
polynom_bck = [4, 4]

  ## Extraction of the spectra:
  # Before the spectra are extracted, the traces of the orders can be shifted. This is necessary for an unstabilised spectrograph, when the orders move in cross-dispersion direction.
  #         format: float or integer
  #         Gives the maximum allowed shift. Set to 0 if shift shouldn't be measured. 
  #         Shift can't be larger as the minimum distance between consecutive orders (single fiber) or between science and calibration orders (bifurcated fiber input).
extraction_shift = 3
  # Orders, which are only partially identified can be ignored for the extraction.
  #         format: int or float between 0 and 100
  #         To disable set extraction_min_ident_part_of_trace_percent to 0. A value of 25 will mean orders that have a length of less than 25% compared to the longest order are ignored.
extraction_min_ident_part_of_trace_percent = 25
  # Precission on how to extract the fractional pixel at the edges of the trace.
  #         standard: using the fraction of the flux of the fractional pixel
  #         linfit:   do a linear fit around the fractional pixel
extraction_precision = standard
  # extraction_width_multiplier times the average width of the gaussian fit to the traces of the orders defines how many pixel of either side to extract.
  #         format: float or integer
  #         if width_percentile < 20 then the *extraction_width_multiplier should probably be 1, otherwise they can be higher
  #         extraction_width_multiplier is for the science traces, arcextraction_width_multiplier for the calibration traces
extraction_width_multiplier = 1
arcextraction_width_multiplier = 1
  # Optional sigmaclipping of the normalised spectrum is possible. Empty to disable. Should be disabled for observed objects with emission features.
  #         format: array of one intger and two floats or integers: sigma to clip absorption lines AND sigma to clip emission lines or cosmics
  #         default: [ 10 , 20 , 4 ] : fit a polynom of 10th order, Don't clip absorption lines (20 sigma), clip emission lines that are stronger than 4 sigma
sigmaclip_spectrum = []
  # In case of a bifurcated fiber, a wavelength solution should be determined for both fibers in order to measure the shift between science and calibration wavelength solution.
  #         If this is not possible, the parameter 'pxshift_between_wavesolutions' can be set to the shift between the solutions.
  #             Positive values mean that the same wavelength in the science fiber has a higher pixel number compared to the calbration fiber.
  #         This value will be overwritten by the entry in the second column with "sci-cal" as fourth column in the file from parameter 'master_wavelengths_shift_filename'
pxshift_between_wavesolutions = 0

  # In order to create a linear wavelength scale for the normalised spectrum, what should the resolution of the scale be in Angstrom. Set to 0 to skip the step
wavelength_scale_resolution = 0.00

  ## Use the extracted flux from the files for blaze correction to correct for the trace or use the fitted flux to correct for the blaze
  #     Options:
  #         flux: use the flux
  #         poly<number of orders>: use a polynom of a certain order, standard is "poly15" for 15th order polynomial
blazercor_function = poly15

  ## general filenames which should be the same for a long time.
  # Path and filename to the badpixel mask. Created by create_badpx_mask.py. Leave empty 
badpx_mask_filename = /data/ronny/Reduced/NA/bad_px_mask.fits

  # (Path and) Filename to the file(s) with the catalog lines. The file must contain tab-separated wavelength, line strength, and element. 
  #             The values can be used as given from the NIST database, e.g. 6550.653 \t 140* \t Ne II
  #             The file will be looked for in the position given, in the reduction folder, and in the HiFLEx folder
  #             If a list of 2 files is given, the first file will be used create a rough wavelength solution and the second file with only stable lines to create the final wavelength solution
reference_catalog = reference_lines_UNe.txt
reference_catalog = reference_lines_ThAr.txt, reference_lines_Th.txt
  # The Script requires Angstrom. Give the multiplier for the wavelength to get Angstrom, e.g. catalog_file_wavelength_muliplier=10 for a reference_catalog in [nm]
catalog_file_wavelength_muliplier = 1
  # Which lines of the catalogue should be used (duplicates are not a problem)
use_catalog_lines = NeI, UI, ThI, ArI, ThII, ArII


  ## Raw data information:
  #-----------------------
raw_data_file_endings   = .fit, .fits
raw_data_imtyp_keyword  = IMAGETYP
raw_data_imtyp_bias     = Bias Frame
raw_data_imtyp_dark     = Dark Frame
raw_data_imtyp_flat     = Flat Frame
raw_data_imtyp_trace1   = NA
raw_data_imtyp_blaze    = NA
raw_data_imtyp_trace2   = NA
raw_data_exptim_keyword = EXPTIME
raw_data_dateobs_keyword = DATE-OBS
raw_data_timezone_cor   = 0
  #  If the header contains one of the keywords given in raw_data_mid_exposure_keys, then that fraction instead of half of the exposure time will be used to calculate the mid-exposure time
  #     First matching keyword will be used. If doesn't exist in the header or is empty, then a fraction of 0.5 (middle of exposure) will be used.
  #     Please note: "HIERARCH" will be removed when python reads the header. The first matching keyword will be used
raw_data_mid_exposure_keys = [HIERARCH ESO INS DET1 TMMEAN, ESO INS DET1 TMMEAN, ESO INS DET2 TMMEAN]
raw_data_object_name_keys  = [OBJECT]
  # HARPS:
# raw_data_imtyp_keyword  = OBJECT
# raw_data_imtyp_bias     = BIAS,BIAS
# raw_data_imtyp_trace1   = LAMP,DARK,TUN
# raw_data_imtyp_trace2   = DARK,LAMP,TUN
# raw_data_imtyp_blaze    = LAMP,LAMP,TUN

  # For each type of calibration filetype (e.g. bias, darks) a <filetype>_calibs_create gives the information which corrections will be applied before the <filetype>_rawfiles are combined.
  #   Corrections will applied in the order given!
  # The <filetype>_calibs_create_g define thereby the general values, which will be assinged by prepare_file_list.py to <filetype>_calibs_create
  #   The following settings are possible (case-insensitive): subframe, badpx_mask, bias, dark, flat, cosmic_ray, background, normalise, combine_mean, combine_sum. Please check the manual for more information
  #   For dark calibration darks with the correct exposure time will be used. 
  #   For cosmic_ray removal deepCR is used and the parameters cosmic_ray_settings need to be set.
bias_calibs_create_g            = [subframe, badpx_mask]
dark_calibs_create_g            = [subframe, badpx_mask, bias]
rflat_calibs_create_g           = subframe, badpx_mask, dark, bias, normalise
  # The file types for the calibrations listed above are applied as CCD image
  # The file types for the calibrations listed below are used to find extracted data
trace1_calibs_create_g          = subframe, badpx_mask, dark, bias
trace2_calibs_create_g          = [subframe, badpx_mask, dark, bias]
arc_calibs_create_g             = [subframe, badpx_mask, dark, bias]
blazecor_calibs_create_g        = [subframe, badpx_mask, dark, bias, rflat, background]
extract_calibs_create_g         = [subframe, badpx_mask, dark, bias, rflat, background]
wavelengthcal_calibs_create_g   = [subframe, badpx_mask]
  # Standard calibrations to be done, if no other information is given
standard_calibs_create = []

  # When reading already processed images from the result path, this defines what calibrations should be done. If one file type should be handled different, then <filetype>_calibs_read can be used.
  # Normally this can be empty.
calibs_read = []

  ## Seetings for cosmic ray removal. The first entry gives the algorithm to be used (only deepCR at the moment), the other parameters give details to the algorithm:
  #     deepCR: 2nd parameter: which model to use. Change to path of the model for your camera. 3rd parameter: threashold, best threshold is highest value that generate mask covering full extent of CR
cosmic_ray_settings = [deepCR, ACS-WFC-F606W-2-32, 0.999]

  ## Getting oject site and object coordinates to perform barycentric correction. If the values are stored in the header, then the header values are used
  #-------------------------------------------------------------------------------
  # Define the observatory site. Longitude is measured to the East (comment/uncomment/delete accordingly). 
  #         Values can be set arbitrarily if the information is stored in the fits-headers (check function get_barycent_cor to add header keywords)
  #         longitude is negative for coordinates west
site        = UH
altitude    = 30
latitude    = 51.7534
longitude   = -0.2401
# site        = TNT
# altitude    = 2457
# latitude    = 18.59055
# longitude   = 98.48655
# site       = INT
# altitude   = 2396
# latitude   = 28.7622
# longitude  = -17.8775
 
  # Define the object coordinates using a file with a list of objects. Needed for barycentric correction and RV calculation. Not necessary if the object information are stored in the header.
  #    The file is created during running prepare_file_list.py
  # The file can be given as full path, or can be located in the result_path or in raw_data_paths  (the first one has higher priority)
  # The file needs to contain the comma separated values (Object name is case sentive for the optional parameters): 
  # Object name , RA (hh:mm:ss.ss), DEC (+-dd:mm:ss.ss), PM RA ("/year), PM DEC ("year), Epoch of the coordinates (only the number), Enable/Disable flag (1 for enabled), 
  #             [optional: Mask (G2,K5,M2), [optional: Velocity width to broaden the lines of the binary mask]]
  # RA and DEC can be also degrees in float
object_file = object_list.txt


  ## Configuration files in which the processing of the raw data fits files is defined
  #-------------------------------------------------------------------------------
  # raw_data_file_list contains the analysis of the raw data files (fiber1 and fiber2, exposure time, what to extract
raw_data_file_list      = file_list_raw_data.txt

  # configfile_fitsfiles contains the information of how to processes the raw data and is created by prepare_file_list.py
configfile_fitsfiles    = fits_conf.txt


  ## Result storage information:
  #-----------------------------
  # The folder where the result data is stored. Use the dot (".") for the current folder. The folders given in the following parameters will be created under result_path folder:
  #               path_extraction, path_extraction_single, path_reduced, path_rv_ceres, path_rv_terra, logging_path
result_path = .

  # In the reduced images mark pixels above max_good_value as saturated
  #           format: float or integer
  # SBIG:         max_good_value = 46000
  # all others:   max_good_value = 63000
max_good_value  = 63000

  ## extracted data:
  #-----------------
  # path for the extracted data (CERES format / single files (including 1d spectrum) / HARPS format for extracted spectrum and wavelength). All paths will be relative to the result_path.
path_extraction             = extracted
path_extraction_single      = extracted/single
path_harpsformat            = extracted/HARPS_format
  # BITPIX of the extracted fits-file (-32, -64, everything else: -64)
extracted_bitpix            = -32

  ## reduced images:
  #-----------------
  # The reduced images can be saved, if a path instead of "NA" is given path_reduced. path_reduced will be relative to the result_path.
path_reduced                = reduced_images
path_reduced                = NA

  ## Standard filenames:
  #---------------------
  # master file for the orders of the scientific traces, of the calibration (arc) traces, and the normalised extracted flat spectrum. Created by hiflex.py
master_trace_sci_filename               = master_traces_sci.fits
master_trace_cal_filename               = master_traces_cal.fits
master_wavelensolution_filename         = master_wavelength.fits
master_blaze_spec_norm_filename         = master_blaze_spec_norm.fits
master_wavelengths_shift_filename       = master_wavelengths_shift.txt

  ## RV analysis: please read the manual to check how to install the packages.
  #--------------
  # Using TERRA to create a template spectrum and then use that template to do the relative radial velocity analysis. Need to point to the PRV.jar file.
  #       If terra_jar_file does not exist on the machine, then no radial velocity analysis using this method will be used.
terra_jar_file                  = /home/exohspec/software/terra/PRV.jar
terra_jar_file                  = /home/ronny/software/terra/PRV.jar
  # csv files for TERRA. path_rv_terra will be relative to the result_path. The csv files will be saved in the <object_name>/data/ folder under path_rv_terra.
  #       It will contain the normalised flux
path_rv_terra                   = terra_rv

  # Using SERVAL to create a template spectrum and then do relative radial velocity against this. The path in which the python and serval folders are located needs to be given.
path_serval                     = /home/exohspec/software/mzechmeister/
path_serval                     = /home/ronny/software/mzechmeister/
    # Output of the SERVAL cross-correlation
path_rv_serval                  = serval_rv

  # Using CERES to do a cross-correlation with template spectra (using the normalised spectrum). Needs to point to the base folder of CERES. 
  #       If path_ceres does not exist on the machine, then no radial velocity analysis using this method will be used.
  # path_rv is used to store the results from the CERES pipeline. path_rv will be relative to the result_path.
path_ceres                      = /home/exohspec/software/ceres/
path_ceres                      = /home/ronny/software/ceres/
  # Output of the CERES cross-correlation
path_rv_ceres                   = ceres_rv

  # What data set from the extracted files should be be used for the RV analysis (TERRA, SERVAL)? List of two integers.
  #       [5,6] : continuum corrected spectrum and noise in continuum; [1,2] : extracted spectrum
dataset_rv_analysis             = [5, 6]


  ## General Parameters
  #--------------------
  # editor defines the program which should be used to allow the user to manipulate raw_data_file_list, e.g. vim, emacs, gedit, kwrite
editor                  = gedit
 # For multiprocessing use at most max_cores_used_pct percent of the available CPU cores, e.g. on an 4 core system "max_cores_used_pct = 80" will use at most 3 cores.
max_cores_used_pct      = 80


  ## Logging:
  #----------
  # logging_path will be relative to the result_path and all parameters starting with "logging_" will be stored in logging_path.
logging_path                        = logging
logging_trace1_binned               = master_trace1_binned.fits
logging_traces_binned               = master_traces_sci_searched.fits
logging_traces_im_binned            = traces_in_master_trace1_binned.png
    # If "*" in logging_traces_im, then it will save a picture with the traces for each extraction, the * will replaced by the filename. This will require about 11MB per file.
logging_traces_im                   = traces_in_*.png
logging_traces_im                   = traces_in_master_traces1.png
logging_traces_bisector             = bisector_traces1.png
logging_find_arc_traces             = arctraces_find.png
logging_arctraces_im                = arctraces_in_master_trace2.png
logging_map_shift_orders            = map_shifts.fits
logging_orig_for_background         = orig_for_background_map.fits
logging_blaze_spec                  = blaze_spectrum.pdf
logging_found_arc_lines             = arc_lines_found.txt
logging_identified_arc_lines        = arc_lines_identified.txt
logging_wavelength_solution_form    = wavelength_solution_form.png
logging_em_lines_gauss_width_form   = wavelength_solution_emmission_lines_gaussian_width.png
logging_em_lines_bisector           = wavelength_solution_emmission_lines_bisector.png
logging_arc_line_identification_residuals       = arc_line_identification_residuals.png
logging_arc_line_identification_residuals_hist  = arc_line_identification_residuals_hist.png
logging_arc_line_identification_spectrum        = arc_line_identification_spectrum.pdf
logging_arc_line_identification_positions       = arc_line_identification_positions.png
logging_crossdispersion_shift       = crossdispersions_shift.txt

